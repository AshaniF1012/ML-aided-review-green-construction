{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f894cd-78d5-4b24-a5f8-71b51cdc1b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c30e7d-23ee-4e73-87d7-62f3c3cf8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5214ae3f-049d-45f0-bf33-6da22711abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.read_csv('relv.csv')\n",
    "df = temp\n",
    "df['keywords'] = df['wosarticle__de']+ \" \" +  df['wosarticle__wc']\n",
    "df['content'] = df['title']+ \" \"+ df['content']+ \" \" +df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f4a66c-6351-4e35-bdfa-602c7f82a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['content'].to_numpy()\n",
    "\n",
    "ids = pd.DataFrame(df['id'], columns = ['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62769db6-faaf-4962-bba0-033aedfc4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation performance base analysis low storey building type structure generally carry building isolate single stand building interaction adjacent building incorporate interaction adjacent building seismic performance risk retrofitting analysis use proper modelling technique seem important mean obtain deviation result conventional analysis result isolated single model structure literature review carry follow topic adjacent building model push analysis earthquake acceleration time series database analysis scaled record time history analysis equation motion beta method fragility curve build performance level aim study evaluation vulnerability risk fragility curve seismic performance adjacent building interact structure result show interact isolate analysis type yield different seismic performance level analysis scale accelogram fragility curve time history analysis adjacent building interaction none analysis scale accelogram fragility curve time history analysis adjacent building interaction none\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(data)\n",
    "print (lemmatized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e946c1d-7b8d-4d60-997e-35c8990de0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evaluation', 'performance', 'base', 'analysis', 'low', 'storey', 'building', 'type', 'structure', 'generally', 'carry', 'building', 'isolate', 'single', 'stand', 'building', 'interaction', 'adjacent', 'building', 'incorporate']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "print (data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5e52f4-fdef-4c6b-a185-e331396163f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['development', 'different', 'base', 'different', 'base', 'dynamic', 'study', 'aim', 'develop', 'fragility', 'curve', 'reinforce_concrete_building', 'different', 'height', 'base', 'dynamic', 'analysis', 'model', 'moment_resist', 'concrete', 'frame', 'use', 'storey', 'frame', 'design', 'base', 'sap', 'program', 'use', 'main', 'analysis', 'tool', 'obtain', 'limit_state', 'then', 'incremental_dynamic_analysis', 'carry', 'ground_motion_record', 'obtain', 'curve', 'compare', 'limit_state', 'maximum', 'allowable', 'drift', 'calculation', 'maximum', 'allowable', 'drift', 'base', 'formula', 'result', 'storey', 'well', 'structural', 'performance', 'same', 'ground_motion', 'compare', 'other', 'storey', 'curve', 'develop', 'consider', 'result', 'parameter', 'base', 'result', 'fragility', 'curve', 'high', 'probability', 'exceed', 'limit_state', 'storey', 'storey', 'low', 'probability', 'author_exclusive_license', 'limit', 'curve', 'motion', 'incremental_dynamic_analysis', 'reinforce_concrete', 'none', 'limit', 'curve', 'motion', 'incremental_dynamic_analysis', 'reinforce_concrete', 'none']\n"
     ]
    }
   ],
   "source": [
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=4, threshold=30) \n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=40)  \n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams (texts):\n",
    "    return ([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams (texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "print (data_bigrams_trigrams[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca2df69-76d0-4ba1-9975-18fb3b902ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    " \n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value =0.035\n",
    "\n",
    "drop_words = []\n",
    "words_missing_in_tfidf = []\n",
    "low_value_words = []\n",
    "rek_words =['building','earthquake','fragility', 'seismic','vulnerability','uncertainty','reinforce_concrete_building','empirical','residential','concrete','masonry', 'nonlinear','case_study','region','numerical','reinforce_concrete_frame','country','statistical','disaster','analytical','ground_motion','regression','shear','material','prediction','reinforce_concrete','nonlinear_time_history','incremental_dynamic_analysis','historical','steel','construction_material','flooding','pushover','local','safety','depth','nonlinear_static','resilience','infrastructure','acceleration','resilient','hospital','low_rise','urban','mechanical','flood','coastal','traditional','wall','software','time_history','human_casualty','hybrid','experiment','preparedness','expert','error','inundation_depth','italian']\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        if id2word[item] not in drop_words:\n",
    "            drop_words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    #new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    #corpus[i] = new_bow\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    new_bow = []\n",
    "    for b in bow:\n",
    "        if (id2word[b[0]] in rek_words) or (id2word[b[0]] not in drop_words):\n",
    "            new_bow.append(b)\n",
    "    corpus[i] = new_bow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955d5b87-e25f-49c7-be09-cf42bae9385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3888993123014293\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams_trigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c44fee7-d995-4b65-8467-7b6a874e10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ba1d42-a97b-40a8-b7d3-2ac1ddb05bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.162*\"ground_motion\" + 0.109*\"reinforce_concrete_frame\" + 0.092*\"high_rise\" + 0.088*\"infill\" + 0.063*\"sequence\" + 0.031*\"reinforce\" + 0.028*\"beam\" + 0.025*\"strong_motion\" + 0.022*\"aftershock\" + 0.017*\"mainshock\"'), (1, '0.265*\"wall\" + 0.132*\"unreinforced_masonry\" + 0.080*\"partition\" + 0.077*\"resistant\" + 0.019*\"inadequate\" + 0.018*\"portion\" + 0.012*\"thorough\" + 0.009*\"pressing\" + 0.008*\"reality\" + 0.006*\"failure_mode\"'), (2, '0.311*\"building\" + 0.263*\"earthquake\" + 0.017*\"low_rise\" + 0.015*\"acceleration\" + 0.011*\"safety\" + 0.011*\"precast\" + 0.010*\"liquefaction\" + 0.010*\"region\" + 0.008*\"disaster\" + 0.007*\"time_history\"'), (3, '0.230*\"vulnerability\" + 0.124*\"building\" + 0.098*\"seismic\" + 0.052*\"masonry\" + 0.040*\"empirical\" + 0.023*\"earthquake\" + 0.012*\"region\" + 0.012*\"urban\" + 0.012*\"unreinforced_masonry_building\" + 0.012*\"country\"'), (4, '0.000*\"ultrasonic_device\" + 0.000*\"pseudo\" + 0.000*\"sustained\" + 0.000*\"repository\" + 0.000*\"publicly\" + 0.000*\"protocol\" + 0.000*\"move_average\" + 0.000*\"lattice\" + 0.000*\"elevated_water_tank\" + 0.000*\"vector_value\"'), (5, '0.245*\"prediction\" + 0.079*\"observation\" + 0.059*\"machine_learning\" + 0.042*\"learning\" + 0.039*\"calculation\" + 0.028*\"wind_speed\" + 0.025*\"rain\" + 0.023*\"forecasting\" + 0.021*\"wind_load\" + 0.020*\"description\"'), (6, '0.255*\"wind\" + 0.137*\"storm\" + 0.070*\"insurance\" + 0.017*\"insurance_company\" + 0.013*\"natural_disaster\" + 0.011*\"translate\" + 0.008*\"windstorm\" + 0.000*\"drive_rain\" + 0.000*\"direction\" + 0.000*\"interior\"'), (7, '0.055*\"bridge\" + 0.052*\"artificial_neural_network\" + 0.043*\"reflect\" + 0.035*\"computational\" + 0.029*\"civil_engineering\" + 0.029*\"train\" + 0.028*\"neural_network\" + 0.027*\"fault\" + 0.027*\"limitation\" + 0.026*\"explicit\"'), (8, '0.374*\"uncertainty\" + 0.043*\"foundation\" + 0.036*\"bayesian\" + 0.033*\"fix\" + 0.024*\"non_ductile\" + 0.023*\"random\" + 0.020*\"bayesian_network\" + 0.016*\"flexibility\" + 0.016*\"uniform\" + 0.014*\"return_period\"'), (9, '0.019*\"domain\" + 0.013*\"deep_learning\" + 0.000*\"term_memory\" + 0.000*\"long_short\" + 0.000*\"incorporation\" + 0.000*\"benchmarking\" + 0.000*\"dimension\" + 0.000*\"encoder\" + 0.000*\"substantially\" + 0.000*\"seamless\"'), (10, '0.234*\"seismic\" + 0.189*\"fragility\" + 0.075*\"building\" + 0.044*\"reinforce_concrete\" + 0.031*\"concrete\" + 0.027*\"nonlinear\" + 0.023*\"ground_motion\" + 0.020*\"reinforce_concrete_building\" + 0.018*\"analytical\" + 0.012*\"material\"'), (11, '0.116*\"frequency\" + 0.110*\"control\" + 0.056*\"representation\" + 0.042*\"real_time\" + 0.041*\"convolutional_neural\" + 0.025*\"nonlinear_time_history\" + 0.017*\"relief\" + 0.014*\"timely\" + 0.013*\"window\" + 0.007*\"play\"'), (12, '0.098*\"industrial\" + 0.078*\"connection\" + 0.061*\"far_field\" + 0.051*\"incremental_dynamic_analysis\" + 0.049*\"roof\" + 0.040*\"collapse_prevention\" + 0.036*\"failure_mode\" + 0.033*\"spatial_distribution\" + 0.032*\"improved\" + 0.029*\"provision\"'), (13, '0.354*\"residential\" + 0.108*\"italian\" + 0.037*\"typological\" + 0.029*\"expect_annual\" + 0.026*\"parametric\" + 0.021*\"reinforcement\" + 0.020*\"place\" + 0.019*\"number_storey\" + 0.016*\"proposal\" + 0.016*\"last_year\"'), (14, '0.000*\"ultrasonic_device\" + 0.000*\"pseudo\" + 0.000*\"sustained\" + 0.000*\"repository\" + 0.000*\"publicly\" + 0.000*\"protocol\" + 0.000*\"move_average\" + 0.000*\"lattice\" + 0.000*\"elevated_water_tank\" + 0.000*\"vector_value\"'), (15, '0.000*\"ultrasonic_device\" + 0.000*\"pseudo\" + 0.000*\"sustained\" + 0.000*\"repository\" + 0.000*\"publicly\" + 0.000*\"protocol\" + 0.000*\"move_average\" + 0.000*\"lattice\" + 0.000*\"elevated_water_tank\" + 0.000*\"vector_value\"'), (16, '0.048*\"historical\" + 0.045*\"stochastic\" + 0.042*\"predictive\" + 0.035*\"error\" + 0.032*\"regression\" + 0.030*\"unit\" + 0.028*\"cumulative\" + 0.024*\"portfolio\" + 0.022*\"isolate\" + 0.022*\"consistent\"'), (17, '0.179*\"flood\" + 0.031*\"building\" + 0.026*\"residential\" + 0.020*\"depth\" + 0.018*\"resilience\" + 0.017*\"flooding\" + 0.014*\"case_study\" + 0.013*\"house\" + 0.012*\"population\" + 0.012*\"statistic\"'), (18, '0.087*\"target\" + 0.070*\"maximum_likelihood\" + 0.066*\"selection\" + 0.051*\"mechanical_property\" + 0.047*\"excitation\" + 0.039*\"density\" + 0.039*\"prototype\" + 0.036*\"linear_static\" + 0.031*\"attribute\" + 0.020*\"benchmark\"'), (19, '0.232*\"tsunami\" + 0.064*\"hurricane\" + 0.057*\"wave\" + 0.044*\"fragility\" + 0.034*\"coast\" + 0.034*\"velocity\" + 0.027*\"debris\" + 0.025*\"inundation_depth\" + 0.025*\"tsunamis\" + 0.024*\"wooden\"')]\n"
     ]
    }
   ],
   "source": [
    "print (lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e81df5-718f-4b4b-bdfd-d4bbd681addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -10.310741735201438\n",
      "\n",
      "Coherence Score:  0.41705248726440824\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams_trigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99916d4f-1499-440b-9fd5-f407d355c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\2113404940.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_var['Topic'][i] = str(topic_num)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\2113404940.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_var['Keywords'][i] = topic_keywords\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\2113404940.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_var[col][i]=round(prop_topic,4)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\2113404940.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  topic_var['id'][i] = ids['id'][i]\n"
     ]
    }
   ],
   "source": [
    "topic_var = pd.DataFrame(0,index=np.arange(len(corpus)),columns = ['id','0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','Topic','Keywords'])\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        topic_var['id'][i] = ids['id'][i]\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            col = str(topic_num)\n",
    "            topic_var[col][i]=round(prop_topic,4)\n",
    "            if j == 0:\n",
    "                topic_var['Topic'][i] = str(topic_num)\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                topic_var['Keywords'][i] = topic_keywords\n",
    "                \n",
    "format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_bigrams_trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5605c6d3-df98-4423-a8b9-3abadb8a36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_var.to_csv('TopicVar.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd0eba21-79dd-455d-bd0b-7858c5f9245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "methoR=pd.read_csv('RelevantMethod.csv')\n",
    "\n",
    "df_merged = pd.merge(topic_var, methoR, left_on='id', right_on='id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59fa0c2d-8f25-4066-910d-b8888ac88afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2 = df_merged.drop(['content','title','1 - Analytical - prediction','1 - Empirical - prediction','1 - Hybrid - prediction','1 - ML - prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbb34b03-8f5f-4b46-be84-60a805e4f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2.to_csv('ToTSNE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32b7d44f-ceab-4561-ba62-ad8bc3aa9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_topics = pd.DataFrame(columns=['id','Earthquake','Flood','Tsunami','Hurricane','RC','Wood','Steel','Masonry','Lowrise','Medrise','Regional','Urban','Residential'])\n",
    "\n",
    "Main_topics['id'] = topic_var['id']\n",
    "Main_topics['Earthquake'] = topic_var['17']\n",
    "Main_topics['Flood'] = topic_var['9']\n",
    "Main_topics['Tsunami'] = topic_var['11']\n",
    "Main_topics['Hurricane'] = topic_var['4']+topic_var['7']+0.83*topic_var['3']\n",
    "Main_topics['RC'] = topic_var['0']+ 0.07*topic_var['17']\n",
    "Main_topics['Wood'] = topic_var['3']+ topic_var['8']\n",
    "Main_topics['Steel'] = topic_var['5']\n",
    "Main_topics['Masonry'] = 0.01*topic_var['1']+ 0.1*topic_var['17']\n",
    "Main_topics['Lowrise'] = topic_var['10']\n",
    "Main_topics['Medrise'] = 0.13*topic_var['0']\n",
    "Main_topics['Regional'] = topic_var['1']\n",
    "Main_topics['Urban'] = topic_var['15']\n",
    "Main_topics['Residential'] = topic_var['16']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94a1fbf4-715c-4ab8-95d4-fbbeb39f5e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Earthquake</th>\n",
       "      <th>Flood</th>\n",
       "      <th>Tsunami</th>\n",
       "      <th>Hurricane</th>\n",
       "      <th>RC</th>\n",
       "      <th>Wood</th>\n",
       "      <th>Steel</th>\n",
       "      <th>Masonry</th>\n",
       "      <th>Lowrise</th>\n",
       "      <th>Medrise</th>\n",
       "      <th>Regional</th>\n",
       "      <th>Urban</th>\n",
       "      <th>Residential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17496</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.073784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15770</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.007885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20395</td>\n",
       "      <td>0.6605</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046235</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.066881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0831</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19715</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.3936</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21380</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.055055</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.078650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>18286</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.052598</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>18075</td>\n",
       "      <td>0.6884</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.064088</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.069846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>18026</td>\n",
       "      <td>0.6666</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153762</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>0.066972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013923</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>21170</td>\n",
       "      <td>0.8803</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.088976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>12873</td>\n",
       "      <td>0.5316</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.037212</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.053160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1138 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Earthquake   Flood  Tsunami  Hurricane        RC    Wood   Steel  \\\n",
       "0     17496      0.7331  0.0000   0.0000   0.000000  0.051317  0.0000  0.0000   \n",
       "1     15770      0.0749  0.8425   0.0000   0.000000  0.005243  0.0000  0.0000   \n",
       "2     20395      0.6605  0.0000   0.0000   0.000000  0.046235  0.0000  0.0000   \n",
       "3     19715      0.1495  0.3936   0.0000   0.000000  0.010465  0.0000  0.0000   \n",
       "4     21380      0.7865  0.0817   0.0000   0.055400  0.055055  0.0000  0.0157   \n",
       "...     ...         ...     ...      ...        ...       ...     ...     ...   \n",
       "1133  18286      0.7514  0.0000   0.0000   0.059500  0.052598  0.0000  0.0000   \n",
       "1134  18075      0.6884  0.0220   0.0000   0.013363  0.064088  0.0161  0.0000   \n",
       "1135  18026      0.6666  0.0000   0.0000   0.000000  0.153762  0.0000  0.0638   \n",
       "1136  21170      0.8803  0.0000   0.0000   0.000000  0.061621  0.0000  0.0000   \n",
       "1137  12873      0.5316  0.0000   0.1935   0.013363  0.037212  0.0161  0.0000   \n",
       "\n",
       "       Masonry  Lowrise   Medrise  Regional   Urban  Residential  \n",
       "0     0.073784      0.0  0.000000    0.0474  0.0000       0.0000  \n",
       "1     0.007885      0.0  0.000000    0.0395  0.0000       0.0142  \n",
       "2     0.066881      0.0  0.000000    0.0831  0.2085       0.0000  \n",
       "3     0.015060      0.0  0.000000    0.0110  0.2341       0.1719  \n",
       "4     0.078650      0.0  0.000000    0.0000  0.0318       0.0000  \n",
       "...        ...      ...       ...       ...     ...          ...  \n",
       "1133  0.076037      0.0  0.000000    0.0897  0.0000       0.0215  \n",
       "1134  0.069846      0.0  0.002067    0.1006  0.0947       0.0000  \n",
       "1135  0.066972      0.0  0.013923    0.0312  0.0000       0.0000  \n",
       "1136  0.088976      0.0  0.000000    0.0946  0.0000       0.0000  \n",
       "1137  0.053160      0.0  0.000000    0.0000  0.0245       0.1651  \n",
       "\n",
       "[1138 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07e36406-0e83-454b-8d45-623bed40e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Earthquake','Flood','Tsunami','Hurricane','RC','Wood','Steel','Masonry','Lowrise','Medrise','Regional','Urban','Residential']\n",
    "#Main_topics['Govern_Topic'] = Main_topics.idxmax(axis = 1)\n",
    "\n",
    "tempdf = Main_topics.drop(['id'], axis =1)\n",
    "\n",
    "Main_topics['Govern_Top'] = tempdf.idxmax(axis = 1)\n",
    "\n",
    "tempdf = Main_topics.drop(['id','RC','Wood','Steel','Masonry','Lowrise','Medrise','Regional','Urban','Residential','Govern_Top'], axis =1)\n",
    "Main_topics['Hazard'] = tempdf.idxmax(axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2042c8ce-b9bf-40ac-9fe8-06d2b6643a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_topics.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325a5ee7-f56a-4a76-8110-a2536956ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\3991194342.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_topics['Material'].iloc[row] = tempdf.iloc[row].idxmax()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\3991194342.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_topics['Type'].iloc[row] = tempdf.iloc[row].idxmax()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\3991194342.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_topics['Locationwise'].iloc[row] = tempdf.iloc[row].idxmax()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14548\\3991194342.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Main_topics['Usage'].iloc[row] = tempdf.iloc[row].idxmax()\n"
     ]
    }
   ],
   "source": [
    "Main_topics['Material'] = \" \"\n",
    "Main_topics['Type'] = \" \"\n",
    "Main_topics['Locationwise'] = \" \"\n",
    "Main_topics['Usage'] = \" \"\n",
    "tempdf = Main_topics.filter(['RC','Wood','Steel','Masonry'],axis = 1)\n",
    "for row in Main_topics.index:\n",
    "    if max(tempdf.iloc[row]) >= 0.0808:\n",
    "        Main_topics['Material'].iloc[row] = tempdf.iloc[row].idxmax()\n",
    "        \n",
    "tempdf = Main_topics.filter(['Lowrise','Medrise'],axis = 1)\n",
    "for row in Main_topics.index:\n",
    "    if max(tempdf.iloc[row]) >= 0.024:\n",
    "        Main_topics['Type'].iloc[row] = tempdf.iloc[row].idxmax()\n",
    "        \n",
    "tempdf = Main_topics.filter(['Regional','Urban'],axis = 1)\n",
    "for row in Main_topics.index:\n",
    "    if max(tempdf.iloc[row]) >= 0.0717:\n",
    "        Main_topics['Locationwise'].iloc[row] = tempdf.iloc[row].idxmax()\n",
    "\n",
    "tempdf = Main_topics.filter(['Residential'],axis = 1)\n",
    "for row in Main_topics.index:\n",
    "    if max(tempdf.iloc[row]) >= 0.05:\n",
    "        Main_topics['Usage'].iloc[row] = tempdf.iloc[row].idxmax()\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d529a0d-d9fc-4efd-8bdb-e819d7814d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_topics['Keywords'] = df_merged2['Keywords']\n",
    "Main_topics['Approach'] = df_merged2['Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3252e5f-653d-461b-8437-a1533b582a7a",
   "metadata": {},
   "source": [
    "Main_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5919233-ab55-461b-8b15-52bdd1075108",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_topics.to_csv('FinalOutTopics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00582c92-a9b6-48b0-8f0b-3f03276ccdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
